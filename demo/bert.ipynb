{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2569b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6ae23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'need', 'to', 'visit', 'the', 'financial', 'bank', 'tomorrow', ';', 'after', 'that', ',', 'we', \"'\", 'll', 'set', 'up', 'a', 'tent', 'by', 'the', 'river', 'bank', ',', 'just', 'across', 'from', 'the', 'bank', 'building', 'where', 'my', 'friend', 'works', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#The sentence\n",
    "#txt = \"I need to visit the bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank where my friend works.\"\n",
    "#txt = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "#Add the special tokens\n",
    "wrangled_txt = '[CLS] ' + txt + ' [SEP]'\n",
    "\n",
    "#tokenization\n",
    "tokenized_txt = tokenizer.tokenize(wrangled_txt)\n",
    "\n",
    "print(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]             101\n",
      "i               1,045\n",
      "need            2,342\n",
      "to              2,000\n",
      "visit           3,942\n",
      "the             1,996\n",
      "financial       3,361\n",
      "bank            2,924\n",
      "tomorrow        4,826\n",
      ";               1,025\n",
      "after           2,044\n",
      "that            2,008\n",
      ",               1,010\n",
      "we              2,057\n",
      "'               1,005\n",
      "ll              2,222\n",
      "set             2,275\n",
      "up              2,039\n",
      "a               1,037\n",
      "tent            9,311\n",
      "by              2,011\n",
      "the             1,996\n",
      "river           2,314\n",
      "bank            2,924\n",
      ",               1,010\n",
      "just            2,074\n",
      "across          2,408\n",
      "from            2,013\n",
      "the             1,996\n",
      "bank            2,924\n",
      "building        2,311\n",
      "where           2,073\n",
      "my              2,026\n",
      "friend          2,767\n",
      "works           2,573\n",
      ".               1,012\n",
      "[SEP]             102\n"
     ]
    }
   ],
   "source": [
    "#get the ids of the tokens\n",
    "ids_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "\n",
    "#Display the tokens\n",
    "for t in zip(tokenized_txt, ids_tokens):\n",
    "    print('{:<12} {:>8,}'.format(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c9cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_txt)\n",
    "#Convert the token IDs and segment IDs into tensors.\n",
    "\n",
    "token_tensor = torch.tensor([ids_tokens])\n",
    "segment_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07b174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model with the weights\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, return_dict = False)\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8844b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/model_doc/bert#bertmodel\n",
    "#The input is of the shape (batch_size, sequence_length)\n",
    "#Compute the output\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_tensor, segment_tensor)\n",
    "hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b3ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 37\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "#The first one is initial embeddings\n",
    "print (\"Number of layers:\", len(hidden_states))\n",
    "layer_ptr = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_ptr]))\n",
    "batch_ptr = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_ptr][batch_ptr]))\n",
    "token_ptr = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_ptr][batch_ptr][token_ptr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7626f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all the layers\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "#remove the batch dimension\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b2689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 13, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1 so that each word contains the 13 layer hidden states\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5868f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 37 x 768\n"
     ]
    }
   ],
   "source": [
    "#sum the last four layers\n",
    "token_vectors_sum = []\n",
    "\n",
    "# token_embeddings is a [35 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vector = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vectors_sum.append(sum_vector)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vectors_sum), len(token_vectors_sum[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b9a7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 i\n",
      "2 need\n",
      "3 to\n",
      "4 visit\n",
      "5 the\n",
      "6 financial\n",
      "7 bank\n",
      "8 tomorrow\n",
      "9 ;\n",
      "10 after\n",
      "11 that\n",
      "12 ,\n",
      "13 we\n",
      "14 '\n",
      "15 ll\n",
      "16 set\n",
      "17 up\n",
      "18 a\n",
      "19 tent\n",
      "20 by\n",
      "21 the\n",
      "22 river\n",
      "23 bank\n",
      "24 ,\n",
      "25 just\n",
      "26 across\n",
      "27 from\n",
      "28 the\n",
      "29 bank\n",
      "30 building\n",
      "31 where\n",
      "32 my\n",
      "33 friend\n",
      "34 works\n",
      "35 .\n",
      "36 [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Display the token\n",
    "for i, t in enumerate(tokenized_txt):\n",
    "  print (i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f837710",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectors = torch.stack(token_vectors_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823ed9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.78\n",
      "Vector similarity for *different* meanings:  0.66\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "#compare the word bank in 7, 23, and 29\n",
    "#txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "same_bank_word = 1 - cosine(token_vectors[7], token_vectors[29])\n",
    "diff_bank_word1 = 1 - cosine(token_vectors[7], token_vectors[23])\n",
    "diff_bank_word2 = 1 - cosine(token_vectors[23], token_vectors[29])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank_word)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word1)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55b5f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_get_context_in_statue(txt, model):\n",
    "    wrangled_txt = '[CLS] ' + txt + ' [SEP]'\n",
    "    tokenized_txt = tokenizer.tokenize(wrangled_txt)\n",
    "    ids_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "    segments_ids = [1] * len(tokenized_txt)\n",
    "    #Convert the token IDs and segment IDs into tensors.\n",
    "\n",
    "    token_tensor = torch.tensor([ids_tokens])\n",
    "    segment_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_tensor, segment_tensor)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    #Concatenate all the layers\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    #remove the batch dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    print(token_embeddings.shape)\n",
    "\n",
    "    # Swap dimensions 0 and 1 so that each word contains the 13 layer hidden states\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_embeddings.size()\n",
    "\n",
    "    #sum the last four layers\n",
    "    token_vectors_sum = []\n",
    "\n",
    "    # token_embeddings is a [35 x 13 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vector = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vectors_sum.append(sum_vector)\n",
    "\n",
    "    print ('Shape is: %d x %d' % (len(token_vectors_sum), len(token_vectors_sum[0])))\n",
    "\n",
    "    return(torch.stack(token_vectors_sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the word bank in 7, 23, and 29\n",
    "#txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "same_bank_word = 1 - cosine(token_vectors[7], token_vectors[29])\n",
    "diff_bank_word1 = 1 - cosine(token_vectors[7], token_vectors[23])\n",
    "diff_bank_word2 = 1 - cosine(token_vectors[23], token_vectors[29])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank_word)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word1)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9ff3c",
   "metadata": {},
   "source": [
    "## What about the language of the Clean Art Act (thinking back to Chevron)\n",
    "\n",
    "- https://www.law.cornell.edu/uscode/text/42/7411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "761be3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "42 U.S. Code § 7411 - Standards of performance for new stationary source\n",
      "(a)Definitions\n",
      "For purposes of this section:\n",
      "(1)The term standard of performance means a standard for emissions of air pollutants which reflects the degree of emission limitation achievable through the application of the best system of emission reduction which (taking into account the cost of achieving such reduction and any nonair quality health and environmental impact and energy requirements) the Administrator determines has been adequately demonstrated.\n",
      "(2)The term new source means any stationary source the construction or modification of which is commenced after the publication of regulations (or if earlier proposed regulations) prescribing a standard of performance under this section which will be applicable to such source.\n",
      "(3)The term stationary source means any building structure facility or installation which emits or may emit any air pollutant. Nothing in subchapter II of this chapter relating to nonroad engines shall be construed to apply to stationary internal combustion engines.\n",
      "(4)The term modification means any physical change in or change in the method of operation of a stationary source which increases the amount of any air pollutant emitted by such source or which results in the emission of any air pollutant not previously emitted.\n",
      "(5)The term owner or operator means any person who owns leases operates controls or supervises a stationary source.\n",
      "(6)The term existing source means any stationary source other than a new source.\n",
      "(7)The term technological system of continuous emission reduction means—\n",
      "(A)a technological process for production or operation by any source which is inherently low-polluting or nonpolluting or\n",
      "(B)a technological system for continuous reduction of the pollution generated by a source before such pollution is emitted into the ambient air including precombustion cleaning or treatment of fuels.\n",
      "(8)A conversion to coal (A) by reason of an order under section 2(a) of the Energy Supply and Environmental Coordination Act of 1974 [15 U.S.C. 792(a)] or any amendment thereto or any subsequent enactment which supersedes such Act [15 U.S.C. 791 et seq.] or (B) which qualifies under section 7413(d)(5)(A)(ii)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uscode_42_7411='''\n",
    "42 U.S. Code § 7411 - Standards of performance for new stationary sources\n",
    "(a)Definitions\n",
    "For purposes of this section:\n",
    "(1)The term “standard of performance” means a standard for emissions of air pollutants which reflects the degree of emission limitation achievable through the application of the best system of emission reduction which (taking into account the cost of achieving such reduction and any nonair quality health and environmental impact and energy requirements) the Administrator determines has been adequately demonstrated.\n",
    "(2)The term “new source” means any stationary source, the construction or modification of which is commenced after the publication of regulations (or, if earlier, proposed regulations) prescribing a standard of performance under this section which will be applicable to such source.\n",
    "(3)The term “stationary source” means any building, structure, facility, or installation which emits or may emit any air pollutant. Nothing in subchapter II of this chapter relating to nonroad engines shall be construed to apply to stationary internal combustion engines.\n",
    "(4)The term “modification” means any physical change in, or change in the method of operation of, a stationary source which increases the amount of any air pollutant emitted by such source or which results in the emission of any air pollutant not previously emitted.\n",
    "(5)The term “owner or operator” means any person who owns, leases, operates, controls, or supervises a stationary source.\n",
    "(6)The term “existing source” means any stationary source other than a new source.\n",
    "(7)The term “technological system of continuous emission reduction” means—\n",
    "(A)a technological process for production or operation by any source which is inherently low-polluting or nonpolluting, or\n",
    "(B)a technological system for continuous reduction of the pollution generated by a source before such pollution is emitted into the ambient air, including precombustion cleaning or treatment of fuels.\n",
    "(8)A conversion to coal (A) by reason of an order under section 2(a) of the Energy Supply and Environmental Coordination Act of 1974 [15 U.S.C. 792(a)] or any amendment thereto, or any subsequent enactment which supersedes such Act [15 U.S.C. 791 et seq.], or (B) which qualifies under section 7413(d)(5)(A)(ii)\n",
    "'''\n",
    "# clean up text to get more hits\n",
    "uscode_42_7411=uscode_42_7411.replace('“','')\n",
    "uscode_42_7411=uscode_42_7411.replace('”','')\n",
    "uscode_42_7411=uscode_42_7411.replace(',','')\n",
    "uscode_42_7411=uscode_42_7411.replace('sources','source')\n",
    "\n",
    "print(uscode_42_7411)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee86c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 472, 768])\n",
      "Shape is: 472 x 768\n"
     ]
    }
   ],
   "source": [
    "cleanairact_token_vectors = bert_get_context_in_statue(uscode_42_7411, \n",
    "                                                       BertModel.from_pretrained('bert-base-uncased', \n",
    "                                                                                 output_hidden_states = True, \n",
    "                                                                                 return_dict = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63f2d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_term='source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b3d031e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 81, 85, 122, 177, 189, 224, 228, 252, 272]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay I probably want to get the index of each occurrence of the ambiguous term\n",
    "ambiguous_term = \"source\"\n",
    "ambiguous_term_indexes = [i for i, token in enumerate(uscode_42_7411.split()) if token == ambiguous_term]\n",
    "ambiguous_term_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a903501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.34\n",
      "... (2)The term new source means any stationary ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.36\n",
      "... means any stationary source the construction or ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.33\n",
      "... (3)The term stationary source means any building ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.32\n",
      "... of a stationary source which increases the ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.20\n",
      "... emitted by such source or which results ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.40\n",
      "... (6)The term existing source means any stationary ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.30\n",
      "... means any stationary source other than a ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.27\n",
      "... operation by any source which is inherently ...\n",
      "\n",
      "Vector similarity for  *similar*  meanings:  0.32\n",
      "... generated by a source before such pollution ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply cosine similarity to find similar meanings for the first index relative to each subsequent. i.e. the title to the statute compared to the language\n",
    "#[12, 81, 85, 122, 177, 189, 224, 228, 252, 272]\n",
    "\n",
    "for i in [12, 81, 85, 122, 177, 189, 224, 228, 252, 272]:\n",
    "    if i == 12:\n",
    "        continue\n",
    "    same_source_word = 1 - cosine(cleanairact_token_vectors[12], cleanairact_token_vectors[i])\n",
    "    print('Vector similarity for  *similar*  meanings:  %.2f' % same_source_word)\n",
    "    # print three tokens before and after each index for the term in a single line, starting and ending with ...\n",
    "    tokens = uscode_42_7411.split()\n",
    "    start = max(0, i - 3)\n",
    "    end = min(len(tokens), i + 4)\n",
    "    context = ' '.join(tokens[start:end])\n",
    "    print(f\"... {context} ...\")\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f22a8",
   "metadata": {},
   "source": [
    "## Using AWS Bedrock with DeepSeek\n",
    "\n",
    "- passing the prompt to the model for a similar AI LLM based task\n",
    "- this compares later to the bert pipeline for filling in the blanks\n",
    "- is really just exploratory on how embedding models work, and how they along with some AI techniques can be used for various NLP tasks in `getout-of-text3`\n",
    "\n",
    "### examples\n",
    "\n",
    "- In finding the ordinary meaning of words, namely the ambiguous text of importance in a statutory interpretation that is up for debate, there are various techniques we can employ to disambiguate the text and extract its intended meaning, including tradition KWIC (COCA), Embedding (LEGAL-BERT), and AI LLMs (DeepSeek on AWS Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83bfcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name='atn-developer')\n",
    "\n",
    "bedrock = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=\"us.deepseek.r1-v1:0\",\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body='{\"prompt\": \"Please analyze the masked sentence to fill the mask: \\\\\"To modify means we should [MASK] significant changes.\\\\\"\", \"max_tokens\": 256}',\n",
    ")\n",
    "\n",
    "deepseek = response['body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13b1c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please provide the answer in the format: [answer] with the most appropriate word to replace [MASK].\n",
      "\n",
      "\n",
      "Okay, let's see. The sentence is \"To modify means we should [MASK] significant changes.\" I need to\n",
      " find the right word to replace [MASK]. The sentence is explaining what \"modify\" means. So, when you\n",
      " modify something, you make changes to it. The verb that goes with \"changes\" here is probably \"make\"\n",
      ". Like, \"make changes\" is a common collocation. Let me think of other possibilities. Maybe \"implemen\n",
      "t\"? But \"implement changes\" is also correct, but does it fit the context? The sentence is defining \"\n",
      "modify\", so the word should be a synonym of \"modify\" in the context of causing changes. \"Make\" is mo\n",
      "re direct and common. \"Create\" could work, but \"make\" is more usual. \"Introduce\" is possible too, bu\n",
      "t again, \"make\" is simpler and more likely. So the best answer is probably \"make\". Let me check agai\n",
      "n. \"To modify means we should make significant changes.\" Yes, that makes sense. The other options mi\n",
      "ght be correct, but \"make\" is the most appropriate and commonly used here\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "deepseek_dict = json.loads(deepseek.decode())\n",
    "#print(deepseek_dict)\n",
    "\n",
    "#deepseek_dict['choices'][0]['text']\n",
    "# print with line wrap\n",
    "# I want to print with a line break in the ['text']\n",
    "text = deepseek_dict['choices'][0]['text']\n",
    "for i in range(0, len(text), 100):\n",
    "    print(text[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d61a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.34469074010849,\n",
       "  'token': 1343,\n",
       "  'token_str': 'vehicle',\n",
       "  'sequence': 'the car is a vehicle that is not permitted in the park.'},\n",
       " {'score': 0.09207320958375931,\n",
       "  'token': 355,\n",
       "  'token_str': 'use',\n",
       "  'sequence': 'the car is a use that is not permitted in the park.'},\n",
       " {'score': 0.050254397094249725,\n",
       "  'token': 2373,\n",
       "  'token_str': 'sign',\n",
       "  'sequence': 'the car is a sign that is not permitted in the park.'},\n",
       " {'score': 0.029583115130662918,\n",
       "  'token': 446,\n",
       "  'token_str': 'service',\n",
       "  'sequence': 'the car is a service that is not permitted in the park.'},\n",
       " {'score': 0.017440086230635643,\n",
       "  'token': 1645,\n",
       "  'token_str': 'car',\n",
       "  'sequence': 'the car is a car that is not permitted in the park.'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"To modify means we should [MASK] significant changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73eac980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1371982991695404,\n",
       "  'token': 4672,\n",
       "  'token_str': 'commodity',\n",
       "  'sequence': 'that bike is a commodity that is not permitted in the park.'},\n",
       " {'score': 0.08565253764390945,\n",
       "  'token': 4175,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'that bike is a game that is not permitted in the park.'},\n",
       " {'score': 0.07633555680513382,\n",
       "  'token': 424,\n",
       "  'token_str': 'product',\n",
       "  'sequence': 'that bike is a product that is not permitted in the park.'},\n",
       " {'score': 0.0717778131365776,\n",
       "  'token': 446,\n",
       "  'token_str': 'service',\n",
       "  'sequence': 'that bike is a service that is not permitted in the park.'},\n",
       " {'score': 0.062396444380283356,\n",
       "  'token': 1343,\n",
       "  'token_str': 'vehicle',\n",
       "  'sequence': 'that bike is a vehicle that is not permitted in the park.'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"That bike is a [MASK] that is not permitted in the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31277aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
