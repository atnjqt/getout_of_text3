{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2569b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6ae23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'need', 'to', 'visit', 'the', 'financial', 'bank', 'tomorrow', ';', 'after', 'that', ',', 'we', \"'\", 'll', 'set', 'up', 'a', 'tent', 'by', 'the', 'river', 'bank', ',', 'just', 'across', 'from', 'the', 'bank', 'building', 'where', 'my', 'friend', 'works', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#The sentence\n",
    "#txt = \"I need to visit the bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank where my friend works.\"\n",
    "#txt = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "#Add the special tokens\n",
    "wrangled_txt = '[CLS] ' + txt + ' [SEP]'\n",
    "\n",
    "#tokenization\n",
    "tokenized_txt = tokenizer.tokenize(wrangled_txt)\n",
    "\n",
    "print(tokenized_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]             101\n",
      "i               1,045\n",
      "need            2,342\n",
      "to              2,000\n",
      "visit           3,942\n",
      "the             1,996\n",
      "financial       3,361\n",
      "bank            2,924\n",
      "tomorrow        4,826\n",
      ";               1,025\n",
      "after           2,044\n",
      "that            2,008\n",
      ",               1,010\n",
      "we              2,057\n",
      "'               1,005\n",
      "ll              2,222\n",
      "set             2,275\n",
      "up              2,039\n",
      "a               1,037\n",
      "tent            9,311\n",
      "by              2,011\n",
      "the             1,996\n",
      "river           2,314\n",
      "bank            2,924\n",
      ",               1,010\n",
      "just            2,074\n",
      "across          2,408\n",
      "from            2,013\n",
      "the             1,996\n",
      "bank            2,924\n",
      "building        2,311\n",
      "where           2,073\n",
      "my              2,026\n",
      "friend          2,767\n",
      "works           2,573\n",
      ".               1,012\n",
      "[SEP]             102\n"
     ]
    }
   ],
   "source": [
    "#get the ids of the tokens\n",
    "ids_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "\n",
    "#Display the tokens\n",
    "for t in zip(tokenized_txt, ids_tokens):\n",
    "    print('{:<12} {:>8,}'.format(t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c9cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_txt)\n",
    "#Convert the token IDs and segment IDs into tensors.\n",
    "\n",
    "token_tensor = torch.tensor([ids_tokens])\n",
    "segment_tensor = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c07b174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model with the weights\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, return_dict = False)\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8844b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/model_doc/bert#bertmodel\n",
    "#The input is of the shape (batch_size, sequence_length)\n",
    "#Compute the output\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_tensor, segment_tensor)\n",
    "hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b3ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 1\n",
      "Number of tokens: 37\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "#The first one is initial embeddings\n",
    "print (\"Number of layers:\", len(hidden_states))\n",
    "layer_ptr = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_ptr]))\n",
    "batch_ptr = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_ptr][batch_ptr]))\n",
    "token_ptr = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_ptr][batch_ptr][token_ptr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7626f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 37, 768])\n"
     ]
    }
   ],
   "source": [
    "#Concatenate all the layers\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "#remove the batch dimension\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b2689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37, 13, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1 so that each word contains the 13 layer hidden states\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5868f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 37 x 768\n"
     ]
    }
   ],
   "source": [
    "#sum the last four layers\n",
    "token_vectors_sum = []\n",
    "\n",
    "# token_embeddings is a [35 x 13 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vector = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vectors_sum.append(sum_vector)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vectors_sum), len(token_vectors_sum[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9a7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 i\n",
      "2 need\n",
      "3 to\n",
      "4 visit\n",
      "5 the\n",
      "6 financial\n",
      "7 bank\n",
      "8 tomorrow\n",
      "9 ;\n",
      "10 after\n",
      "11 that\n",
      "12 ,\n",
      "13 we\n",
      "14 '\n",
      "15 ll\n",
      "16 set\n",
      "17 up\n",
      "18 a\n",
      "19 tent\n",
      "20 by\n",
      "21 the\n",
      "22 river\n",
      "23 bank\n",
      "24 ,\n",
      "25 just\n",
      "26 across\n",
      "27 from\n",
      "28 the\n",
      "29 bank\n",
      "30 building\n",
      "31 where\n",
      "32 my\n",
      "33 friend\n",
      "34 works\n",
      "35 .\n",
      "36 [SEP]\n"
     ]
    }
   ],
   "source": [
    "#Display the token\n",
    "for i, t in enumerate(tokenized_txt):\n",
    "  print (i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f837710",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectors = torch.stack(token_vectors_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "823ed9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.78\n",
      "Vector similarity for *different* meanings:  0.66\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "#compare the word bank in 7, 23, and 29\n",
    "#txt = \"I need to visit the financial bank tomorrow; after that, we'll set up a tent by the river bank, just across from the bank building where my friend works.\"\n",
    "\n",
    "same_bank_word = 1 - cosine(token_vectors[7], token_vectors[29])\n",
    "diff_bank_word1 = 1 - cosine(token_vectors[7], token_vectors[23])\n",
    "diff_bank_word2 = 1 - cosine(token_vectors[23], token_vectors[29])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank_word)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word1)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f22a8",
   "metadata": {},
   "source": [
    "## Using AWS Bedrock with DeepSeek\n",
    "\n",
    "- passing the prompt to the model for a similar AI LLM based task\n",
    "- this compares later to the bert pipeline for filling in the blanks\n",
    "- is really just exploratory on how embedding models work, and how they along with some AI techniques can be used for various NLP tasks in `getout-of-text3`\n",
    "\n",
    "### examples\n",
    "\n",
    "- In finding the ordinary meaning of words, namely the ambiguous text of importance in a statutory interpretation that is up for debate, there are various techniques we can employ to disambiguate the text and extract its intended meaning, including tradition KWIC (COCA), Embedding (LEGAL-BERT), and AI LLMs (DeepSeek on AWS Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83bfcd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name='atn-developer')\n",
    "\n",
    "bedrock = session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    modelId=\"us.deepseek.r1-v1:0\",\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\",\n",
    "    body='{\"prompt\": \"Please analyze the masked sentence to fill the mask: \\\\\"To modify means we should [MASK] significant changes.\\\\\"\", \"max_tokens\": 256}',\n",
    ")\n",
    "\n",
    "deepseek = response['body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13b1c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please provide the answer in the format: [answer] with the most appropriate word to replace [MASK].\n",
      "\n",
      "\n",
      "Okay, let's see. The sentence is \"To modify means we should [MASK] significant changes.\" I need to\n",
      " find the right word to replace [MASK]. The sentence is explaining what \"modify\" means. So, when you\n",
      " modify something, you make changes to it. The verb that goes with \"changes\" here is probably \"make\"\n",
      ". Like, \"make changes\" is a common collocation. Let me think of other possibilities. Maybe \"implemen\n",
      "t\"? But \"implement changes\" is also correct, but does it fit the context? The sentence is defining \"\n",
      "modify\", so the word should be a synonym of \"modify\" in the context of causing changes. \"Make\" is mo\n",
      "re direct and common. \"Create\" could work, but \"make\" is more usual. \"Introduce\" is possible too, bu\n",
      "t again, \"make\" is simpler and more likely. So the best answer is probably \"make\". Let me check agai\n",
      "n. \"To modify means we should make significant changes.\" Yes, that makes sense. The other options mi\n",
      "ght be correct, but \"make\" is the most appropriate and commonly used here\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "deepseek_dict = json.loads(deepseek.decode())\n",
    "#print(deepseek_dict)\n",
    "\n",
    "#deepseek_dict['choices'][0]['text']\n",
    "# print with line wrap\n",
    "# I want to print with a line break in the ['text']\n",
    "text = deepseek_dict['choices'][0]['text']\n",
    "for i in range(0, len(text), 100):\n",
    "    print(text[i:i+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d61a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.34469074010849,\n",
       "  'token': 1343,\n",
       "  'token_str': 'vehicle',\n",
       "  'sequence': 'the car is a vehicle that is not permitted in the park.'},\n",
       " {'score': 0.09207320958375931,\n",
       "  'token': 355,\n",
       "  'token_str': 'use',\n",
       "  'sequence': 'the car is a use that is not permitted in the park.'},\n",
       " {'score': 0.050254397094249725,\n",
       "  'token': 2373,\n",
       "  'token_str': 'sign',\n",
       "  'sequence': 'the car is a sign that is not permitted in the park.'},\n",
       " {'score': 0.029583115130662918,\n",
       "  'token': 446,\n",
       "  'token_str': 'service',\n",
       "  'sequence': 'the car is a service that is not permitted in the park.'},\n",
       " {'score': 0.017440086230635643,\n",
       "  'token': 1645,\n",
       "  'token_str': 'car',\n",
       "  'sequence': 'the car is a car that is not permitted in the park.'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"To modify means we should [MASK] significant changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73eac980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1371982991695404,\n",
       "  'token': 4672,\n",
       "  'token_str': 'commodity',\n",
       "  'sequence': 'that bike is a commodity that is not permitted in the park.'},\n",
       " {'score': 0.08565253764390945,\n",
       "  'token': 4175,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'that bike is a game that is not permitted in the park.'},\n",
       " {'score': 0.07633555680513382,\n",
       "  'token': 424,\n",
       "  'token_str': 'product',\n",
       "  'sequence': 'that bike is a product that is not permitted in the park.'},\n",
       " {'score': 0.0717778131365776,\n",
       "  'token': 446,\n",
       "  'token_str': 'service',\n",
       "  'sequence': 'that bike is a service that is not permitted in the park.'},\n",
       " {'score': 0.062396444380283356,\n",
       "  'token': 1343,\n",
       "  'token_str': 'vehicle',\n",
       "  'sequence': 'that bike is a vehicle that is not permitted in the park.'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"That bike is a [MASK] that is not permitted in the park.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31277aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
