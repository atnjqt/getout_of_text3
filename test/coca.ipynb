{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6c1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: getout_of_text_3 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (0.1.3)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from getout_of_text_3) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from pandas>=1.0->getout_of_text_3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ejacquot/Documents/Github/getout_of_text_3/.venv_dev/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0->getout_of_text_3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install getout_of_text_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2125a2c",
   "metadata": {},
   "source": [
    "### Demonstration notebook for functionalities to include on GOT3 tool\n",
    "\n",
    "- working through some steps with `pandas` and`nltk`, to later roll into the toolset `getout_of_text_3` to streamline COCA Corpora searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa657b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getout_of_text_3 as got3\n",
    "got3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b2d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import necessary libraries for search functionality\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e447c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK libraries ready!\n",
      "‚úÖ Tokenization working: ['This', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"‚úÖ NLTK libraries ready!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è NLTK download may have failed, but will continue\")\n",
    "\n",
    "# Test tokenization\n",
    "try:\n",
    "    test_words = nltk.word_tokenize(\"This is a test.\")\n",
    "    print(f\"‚úÖ Tokenization working: {test_words}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Tokenization issue: {e}\")\n",
    "    print(\"Will use simple split() method as fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d6a8",
   "metadata": {},
   "source": [
    "## reading coca db and txt\n",
    "- dictionaries with genre as the key and dfs as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c516bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = ['acad', 'blog', 'fic', \n",
    "              'mag', 'news', 'spok',\n",
    "              'tvm', 'web']\n",
    "db_df = {}\n",
    "db_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe00fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files for... acad\n",
      "  db_acad.txt: (1419500, 1)\n",
      "  text_acad.txt: (265, 1)\n",
      "Reading files for... blog\n",
      "  db_acad.txt: (1419500, 1)\n",
      "  text_acad.txt: (265, 1)\n",
      "Reading files for... blog\n",
      "  db_blog.txt: (1586094, 1)\n",
      "  text_blog.txt: (991, 1)\n",
      "Reading files for... fic\n",
      "  db_blog.txt: (1586094, 1)\n",
      "  text_blog.txt: (991, 1)\n",
      "Reading files for... fic\n",
      "  db_fic.txt: (1405902, 1)\n",
      "  text_fic.txt: (273, 1)\n",
      "Reading files for... mag\n",
      "  db_fic.txt: (1405902, 1)\n",
      "  text_fic.txt: (273, 1)\n",
      "Reading files for... mag\n",
      "  db_mag.txt: (1567102, 1)\n",
      "  text_mag.txt: (948, 1)\n",
      "Reading files for... news\n",
      "  db_mag.txt: (1567102, 1)\n",
      "  text_mag.txt: (948, 1)\n",
      "Reading files for... news\n",
      "  db_news.txt: (1389753, 1)\n",
      "  text_news.txt: (871, 1)\n",
      "Reading files for... spok\n",
      "  db_news.txt: (1389753, 1)\n",
      "  text_news.txt: (871, 1)\n",
      "Reading files for... spok\n",
      "  db_spok.txt: (1160506, 1)\n",
      "  text_spok.txt: (263, 1)\n",
      "Reading files for... tvm\n",
      "  db_spok.txt: (1160506, 1)\n",
      "  text_spok.txt: (263, 1)\n",
      "Reading files for... tvm\n",
      "  db_tvm.txt: (1567561, 1)\n",
      "  text_tvm.txt: (233, 1)\n",
      "Reading files for... web\n",
      "  db_tvm.txt: (1567561, 1)\n",
      "  text_tvm.txt: (233, 1)\n",
      "Reading files for... web\n",
      "  db_web.txt: (1423557, 1)\n",
      "  text_web.txt: (892, 1)\n",
      "\n",
      "‚úÖ Successfully created two dictionaries of DataFrames from COCA sample files\n",
      "   - db_df: 8 genres loaded\n",
      "   - db_text: 8 genres loaded\n",
      "  db_web.txt: (1423557, 1)\n",
      "  text_web.txt: (892, 1)\n",
      "\n",
      "‚úÖ Successfully created two dictionaries of DataFrames from COCA sample files\n",
      "   - db_df: 8 genres loaded\n",
      "   - db_text: 8 genres loaded\n"
     ]
    }
   ],
   "source": [
    "for genre in genre_dict:\n",
    "    print(f\"üìÇ Processing {genre}...\")\n",
    "    \n",
    "    # Load db file\n",
    "    try:\n",
    "        db_df[genre] = pd.read_csv(\"../coca-samples-db/db_{}.txt\".format(genre), \n",
    "                                   sep=\"\\t\", \n",
    "                                   header=None, \n",
    "                                   names=[\"text\"],\n",
    "                                   on_bad_lines='skip',\n",
    "                                   quoting=3)\n",
    "        print(f\"  ‚úÖ db_{genre}.txt: {db_df[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error reading db_{genre}: {e}\")\n",
    "    \n",
    "    # Load text file\n",
    "    try:\n",
    "        db_text[genre] = pd.read_csv(\"../coca-samples-text/text_{}.txt\".format(genre), \n",
    "                                     sep=\"\\t\", \n",
    "                                     header=None, \n",
    "                                     names=[\"text\"],\n",
    "                                     on_bad_lines='skip',\n",
    "                                     quoting=3)\n",
    "        print(f\"  ‚úÖ text_{genre}.txt: {db_text[genre].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error reading text_{genre}: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ SUMMARY:\")\n",
    "print(f\"   - db_df: {len(db_df)} genres loaded\") \n",
    "print(f\"   - db_text: {len(db_text)} genres loaded\")\n",
    "print(f\"   - Processed each genre exactly once ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5f585",
   "metadata": {},
   "source": [
    "## For collocate or keyword searches, we can use the following approach:\n",
    "1. loop through each genre\n",
    "2. do string filter hits for each instance of a string match in the dictionary key dataframe text column\n",
    "3. print out in an elegant manner\n",
    "\n",
    "use NLTK for this if that makes things easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da5cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Search functions created successfully!\n"
     ]
    }
   ],
   "source": [
    "def search_keyword_corpus(keyword, db_dict, case_sensitive=False, show_context=True, context_words=5):\n",
    "    \"\"\"\n",
    "    Search for a keyword across all COCA genres and display results elegantly.\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: The word/phrase to search for\n",
    "    - db_dict: Dictionary of DataFrames (either db_df or db_text)\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    - show_context: Whether to show surrounding context\n",
    "    - context_words: Number of words to show on each side for context\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with search results by genre\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç COCA Corpus Search: '{keyword}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    total_hits = 0\n",
    "    \n",
    "    # Prepare search pattern\n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Search through each genre\n",
    "    for genre, df in db_dict.items():\n",
    "        genre_hits = 0\n",
    "        print(f\"\\nüìö {genre.upper()} Genre:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for idx, text in df['text'].items():\n",
    "            text_str = str(text)\n",
    "            matches = pattern.findall(text_str)\n",
    "            \n",
    "            if matches:\n",
    "                genre_hits += len(matches)\n",
    "                \n",
    "                if show_context:\n",
    "                    # Find all match positions and show context\n",
    "                    for match in pattern.finditer(text_str):\n",
    "                        start, end = match.span()\n",
    "                        \n",
    "                        # Get context words\n",
    "                        words = text_str.split()\n",
    "                        text_words = ' '.join(words)\n",
    "                        \n",
    "                        # Find word boundaries for context\n",
    "                        words_before_match = text_str[:start].split()\n",
    "                        words_after_match = text_str[end:].split()\n",
    "                        \n",
    "                        # Build context\n",
    "                        context_before = ' '.join(words_before_match[-context_words:]) if words_before_match else \"\"\n",
    "                        matched_word = text_str[start:end]\n",
    "                        context_after = ' '.join(words_after_match[:context_words]) if words_after_match else \"\"\n",
    "                        \n",
    "                        # Format the context nicely\n",
    "                        context_display = f\"...{context_before} **{matched_word}** {context_after}...\"\n",
    "                        context_display = context_display.replace(\"...\", \"\").strip()\n",
    "                        \n",
    "                        results[genre].append({\n",
    "                            'text_id': idx,\n",
    "                            'match': matched_word,\n",
    "                            'context': context_display,\n",
    "                            'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  üìù Text {idx}: {context_display}\")\n",
    "                else:\n",
    "                    results[genre].append({\n",
    "                        'text_id': idx,\n",
    "                        'matches': len(matches),\n",
    "                        'full_text': text_str[:100] + \"...\" if len(text_str) > 100 else text_str\n",
    "                    })\n",
    "        \n",
    "        if genre_hits > 0:\n",
    "            print(f\"  ‚úÖ Found {genre_hits} occurrence(s) in {genre}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå No matches found in {genre}\")\n",
    "            \n",
    "        total_hits += genre_hits\n",
    "    \n",
    "    print(f\"\\nüéØ SUMMARY:\")\n",
    "    print(f\"Total hits across all genres: {total_hits}\")\n",
    "    print(f\"Genres with matches: {len([g for g in results if results[g]])}\")\n",
    "    \n",
    "    return dict(results)\n",
    "\n",
    "# Helper function for frequency analysis\n",
    "def keyword_frequency_analysis(keyword, db_dict, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Analyze frequency of keyword across genres\n",
    "    \"\"\"\n",
    "    print(f\"üìä Frequency Analysis for '{keyword}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    freq_data = {}\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    for genre, df in db_dict.items():\n",
    "        total_words = 0\n",
    "        keyword_count = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text)\n",
    "            words = text_str.split()\n",
    "            total_words += len(words)\n",
    "            keyword_count += len(pattern.findall(text_str))\n",
    "        \n",
    "        # Calculate frequency per 1000 words\n",
    "        freq_per_1000 = (keyword_count / total_words * 1000) if total_words > 0 else 0\n",
    "        \n",
    "        freq_data[genre] = {\n",
    "            'count': keyword_count,\n",
    "            'total_words': total_words,\n",
    "            'freq_per_1000': round(freq_per_1000, 3)\n",
    "        }\n",
    "        \n",
    "        print(f\"{genre:8s}: {keyword_count:4d} occurrences | {freq_per_1000:6.3f} per 1000 words\")\n",
    "    \n",
    "    return freq_data\n",
    "\n",
    "print(\"‚úÖ Search functions created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65513ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COCA Corpus Search: 'textual'\n",
      "============================================================\n",
      "\n",
      "üìö ACAD Genre:\n",
      "------------------------------\n",
      "  üìù Text 4: New Testaments , of multiple **textual** layers . In fact ,\n",
      "  üìù Text 4: little evolving world of complex **textual** strata . As in The\n",
      "  üìù Text 4: its own complicated pastiche of **textual** fragments recounted by at least\n",
      "  üìù Text 31: basis for multiple languages -- **textual** , graphic , photographic ,\n",
      "  üìù Text 73: describes television as \" the **textual** technology of information theory ,\n",
      "  üìù Text 78: a chorus-commentary underline the irritating **textual** bombardment , while a stereophonic\n",
      "  üìù Text 87: distinction between an intrinsic , **textual** \" you \" -- a\n",
      "  üìù Text 87: this page \" is both **textual** and extratextual : it refers\n",
      "  üìù Text 87: play with the location ( **textual** and/or extratextual ) of the\n",
      "  üìù Text 87: there is some evidence ( **textual** or historical ) to the\n",
      "  üìù Text 87: there is some evidence ( **textual** or historical ) to the\n",
      "  üìù Text 87: that involves discussion of the **textual** grounds for those experiences ,\n",
      "  üìù Text 88: , impersonal , nuclear , **textual** , performative , botanical ,\n",
      "  üìù Text 163: the Koran course and the **textual** passages that formed the basis\n",
      "  üìù Text 233: reminders for thematic issues and **textual** analysis revealed through classroom dialogue\n",
      "  üìù Text 247: as well as in the **textual** , visual , and epistemological\n",
      "  üìù Text 247: . \" Each of my **textual** examples , Elizabeth Gaskell 's\n",
      "  üìù Text 249: after finding ambiguity as to **textual** meaning.42 Second , if there\n",
      "  üìù Text 249: is a conflict between the **textual** meaning of a provision and\n",
      "  üìù Text 249: 314 and having a clear **textual** basis is neither a necessary\n",
      "  üìù Text 249: of a constitutional provision whose **textual** meaning is fairly clear but\n",
      "  ‚úÖ Found 21 occurrence(s) in acad\n",
      "\n",
      "üìö BLOG Genre:\n",
      "------------------------------\n",
      "  üìù Text 263: subject and in providing a **textual** exegesis to the far-reaching deductions\n",
      "  üìù Text 980: hyperlinks , social bookmarks , **textual** content , graphics and also\n",
      "  ‚úÖ Found 2 occurrence(s) in blog\n",
      "\n",
      "üìö FIC Genre:\n",
      "------------------------------\n",
      "  üìù Text 67: , a novel whose very **textual** structure disrupts all notions of\n",
      "  ‚úÖ Found 1 occurrence(s) in fic\n",
      "\n",
      "üìö MAG Genre:\n",
      "------------------------------\n",
      "  ‚ùå No matches found in mag\n",
      "\n",
      "üìö NEWS Genre:\n",
      "------------------------------\n",
      "  ‚ùå No matches found in news\n",
      "\n",
      "üìö SPOK Genre:\n",
      "------------------------------\n",
      "  ‚ùå No matches found in spok\n",
      "\n",
      "üìö TVM Genre:\n",
      "------------------------------\n",
      "  ‚ùå No matches found in tvm\n",
      "\n",
      "üìö WEB Genre:\n",
      "------------------------------\n",
      "  üìù Text 48: @ @ @ @ @ **textual** matter , if any ,\n",
      "  üìù Text 571: that violates every rule of **textual** analysis and interpretation . If\n",
      "  ‚úÖ Found 2 occurrence(s) in web\n",
      "\n",
      "üéØ SUMMARY:\n",
      "Total hits across all genres: 26\n",
      "Genres with matches: 4\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Search for a legal term across all genres\n",
    "keyword = \"textual\"\n",
    "search_results = search_keyword_corpus(keyword, db_text, case_sensitive=False, show_context=True, context_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caf234ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Frequency Analysis for 'textual'\n",
      "==================================================\n",
      "acad    :   21 occurrences |  0.015 per 1000 words\n",
      "blog    :    2 occurrences |  0.001 per 1000 words\n",
      "fic     :    1 occurrences |  0.001 per 1000 words\n",
      "mag     :    0 occurrences |  0.000 per 1000 words\n",
      "fic     :    1 occurrences |  0.001 per 1000 words\n",
      "mag     :    0 occurrences |  0.000 per 1000 words\n",
      "news    :    0 occurrences |  0.000 per 1000 words\n",
      "spok    :    0 occurrences |  0.000 per 1000 words\n",
      "tvm     :    0 occurrences |  0.000 per 1000 words\n",
      "news    :    0 occurrences |  0.000 per 1000 words\n",
      "spok    :    0 occurrences |  0.000 per 1000 words\n",
      "tvm     :    0 occurrences |  0.000 per 1000 words\n",
      "web     :    2 occurrences |  0.001 per 1000 words\n",
      "web     :    2 occurrences |  0.001 per 1000 words\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Frequency analysis across genres\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "freq_results = keyword_frequency_analysis(keyword, db_text, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82238f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîß CLEANED COLLOCATE ANALYSIS:\n",
      "üîó Collocate Analysis for 'help' (window: ¬±3 words)\n",
      "============================================================\n",
      "\n",
      "üìö ACAD Genre Collocates:\n",
      "  Found 434 instances of 'help' in acad\n",
      "  the            : 126 times\n",
      "  and            :  67 times\n",
      "  can            :  56 times\n",
      "  that           :  42 times\n",
      "  with           :  36 times\n",
      "  students       :  36 times\n",
      "  will           :  28 times\n",
      "  may            :  27 times\n",
      "  for            :  27 times\n",
      "  them           :  20 times\n",
      "\n",
      "üìö BLOG Genre Collocates:\n",
      "  Found 701 instances of 'help' in blog\n",
      "  the            : 189 times\n",
      "  you            :  98 times\n",
      "  and            :  97 times\n",
      "  that           :  69 times\n",
      "  can            :  55 times\n",
      "  with           :  51 times\n",
      "  will           :  50 times\n",
      "  for            :  47 times\n",
      "  get            :  33 times\n",
      "  your           :  33 times\n",
      "\n",
      "üìö FIC Genre Collocates:\n",
      "  Found 412 instances of 'help' in fic\n",
      "  the            :  80 times\n",
      "  you            :  74 times\n",
      "  could          :  44 times\n",
      "  and            :  41 times\n",
      "  with           :  32 times\n",
      "  but            :  32 times\n",
      "  can            :  29 times\n",
      "  him            :  27 times\n",
      "  for            :  27 times\n",
      "  her            :  24 times\n",
      "\n",
      "üìö MAG Genre Collocates:\n",
      "  Found 666 instances of 'help' in mag\n",
      "  the            : 173 times\n",
      "  you            : 116 times\n",
      "  can            :  75 times\n",
      "  and            :  70 times\n",
      "  will           :  56 times\n",
      "  with           :  55 times\n",
      "  that           :  54 times\n",
      "  them           :  39 times\n",
      "  your           :  38 times\n",
      "  for            :  37 times\n",
      "\n",
      "üìö NEWS Genre Collocates:\n",
      "  Found 500 instances of 'help' in news\n",
      "  the            : 159 times\n",
      "  and            :  55 times\n",
      "  that           :  47 times\n",
      "  with           :  33 times\n",
      "  for            :  28 times\n",
      "  will           :  27 times\n",
      "  can            :  27 times\n",
      "  but            :  26 times\n",
      "  you            :  26 times\n",
      "  people         :  23 times\n",
      "\n",
      "üìö SPOK Genre Collocates:\n",
      "  Found 460 instances of 'help' in spok\n",
      "  the            : 122 times\n",
      "  you            :  54 times\n",
      "  that           :  45 times\n",
      "  with           :  43 times\n",
      "  and            :  42 times\n",
      "  for            :  41 times\n",
      "  can            :  32 times\n",
      "  out            :  32 times\n",
      "  will           :  31 times\n",
      "  them           :  30 times\n",
      "\n",
      "üìö TVM Genre Collocates:\n",
      "  Found 1165 instances of 'help' in tvm\n",
      "  you            : 395 times\n",
      "  can            : 130 times\n",
      "  help           : 114 times\n",
      "  need           :  98 times\n",
      "  the            :  96 times\n",
      "  your           :  78 times\n",
      "  with           :  70 times\n",
      "  out            :  56 times\n",
      "  that           :  55 times\n",
      "  could          :  54 times\n",
      "\n",
      "üìö WEB Genre Collocates:\n",
      "  Found 701 instances of 'help' in web\n",
      "  the            : 167 times\n",
      "  you            : 105 times\n",
      "  and            : 102 times\n",
      "  can            :  67 times\n",
      "  will           :  62 times\n",
      "  that           :  55 times\n",
      "  with           :  52 times\n",
      "  for            :  45 times\n",
      "  your           :  40 times\n",
      "  but            :  34 times\n",
      "\n",
      "üéØ TOP OVERALL COLLOCATES (min frequency: 1):\n",
      "----------------------------------------\n",
      "the            : 1112 occurrences\n",
      "you            : 885 occurrences\n",
      "and            : 527 occurrences\n",
      "can            : 471 occurrences\n",
      "that           : 389 occurrences\n",
      "with           : 372 occurrences\n",
      "for            : 292 occurrences\n",
      "will           : 285 occurrences\n",
      "your           : 235 occurrences\n",
      "could          : 225 occurrences\n",
      "need           : 216 occurrences\n",
      "them           : 203 occurrences\n",
      "but            : 199 occurrences\n",
      "out            : 195 occurrences\n",
      "get            : 174 occurrences\n",
      "this           : 159 occurrences\n",
      "would          : 144 occurrences\n",
      "they           : 140 occurrences\n",
      "him            : 137 occurrences\n",
      "help           : 136 occurrences\n"
     ]
    }
   ],
   "source": [
    "def find_collocates(keyword, db_dict, window_size=5, min_freq=2, case_sensitive=False):\n",
    "    \"\"\"\n",
    "    Find words that frequently appear near the keyword (collocates)\n",
    "    \n",
    "    Parameters:\n",
    "    - keyword: Target word to find collocates for\n",
    "    - db_dict: Dictionary of DataFrames\n",
    "    - window_size: Number of words to look at on each side\n",
    "    - min_freq: Minimum frequency for a word to be considered a collocate\n",
    "    - case_sensitive: Whether to perform case-sensitive search\n",
    "    \"\"\"\n",
    "    print(f\"üîó Collocate Analysis for '{keyword}' (window: ¬±{window_size} words)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if case_sensitive:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b')\n",
    "    else:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "    \n",
    "    all_collocates = Counter()\n",
    "    genre_collocates = {}\n",
    "\n",
    "    for genre, df in db_dict.items():\n",
    "        print(f\"\\nüìö {genre.upper()} Genre Collocates:\")\n",
    "        \n",
    "        # Create a fresh counter for each genre\n",
    "        genre_counter = Counter()\n",
    "        keyword_instances = 0\n",
    "        \n",
    "        for text in df['text']:\n",
    "            text_str = str(text).lower() if not case_sensitive else str(text)\n",
    "            words = nltk.word_tokenize(text_str)\n",
    "            \n",
    "            # Find all positions of the keyword\n",
    "            keyword_positions = []\n",
    "            for i, word in enumerate(words):\n",
    "                if (not case_sensitive and word.lower() == keyword.lower()) or (case_sensitive and word == keyword):\n",
    "                    keyword_positions.append(i)\n",
    "            \n",
    "            keyword_instances += len(keyword_positions)\n",
    "            \n",
    "            # Extract collocates around each keyword occurrence\n",
    "            for pos in keyword_positions:\n",
    "                start = max(0, pos - window_size)\n",
    "                end = min(len(words), pos + window_size + 1)\n",
    "                \n",
    "                # Get surrounding words (excluding the keyword itself)\n",
    "                context_words = words[start:pos] + words[pos+1:end]\n",
    "                \n",
    "                # Filter out punctuation and very short words\n",
    "                context_words = [w for w in context_words if w.isalpha() and len(w) > 2]\n",
    "                \n",
    "                genre_counter.update(context_words)\n",
    "                all_collocates.update(context_words)\n",
    "        \n",
    "        # Store the results for this genre\n",
    "        genre_collocates[genre] = genre_counter\n",
    "        \n",
    "        # Display top collocates for this genre\n",
    "        top_collocates = genre_counter.most_common(10)\n",
    "        if top_collocates:\n",
    "            print(f\"  Found {keyword_instances} instances of '{keyword}' in {genre}\")\n",
    "            # Show all results, but mark those below min_freq\n",
    "            for word, freq in top_collocates:\n",
    "                marker = \"  \" if freq >= min_freq else \"* \"\n",
    "                print(f\"{marker}{word:15s}: {freq:3d} times\")\n",
    "        else:\n",
    "            print(f\"  Found {keyword_instances} instances, but no significant collocates\")\n",
    "    \n",
    "    print(f\"\\nüéØ TOP OVERALL COLLOCATES (min frequency: {min_freq}):\")\n",
    "    print(\"-\" * 40)\n",
    "    top_overall = all_collocates.most_common(20)\n",
    "    for word, freq in top_overall:\n",
    "        if freq >= min_freq:\n",
    "            print(f\"{word:15s}: {freq:3d} occurrences\")\n",
    "        \n",
    "    return {\n",
    "        'all_collocates': dict(all_collocates),\n",
    "        'by_genre': dict(genre_collocates),\n",
    "        #'top_overall': top_overall\n",
    "    }\n",
    "\n",
    "# Example 3: Find collocates for the keyword (CLEANED VERSION)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß CLEANED COLLOCATE ANALYSIS:\")\n",
    "collocate_results = find_collocates('help', db_text, window_size=3, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74c346",
   "metadata": {},
   "source": [
    "### think about how it's helpful to have these various results displayed / consumed by the user\n",
    "\n",
    "The `got3` tool should have the following included steps for ease of access with working with COCA from BYU\n",
    "\n",
    "1. read the database files `got3.read_corpora(dir_of_text_files,corpora_name)`\n",
    "2. perform collocate analysis using `got3.find_collocates(keyword, db_dict, window_size, min_freq, case_sensitive)`\n",
    "3. perform keyword search using `got3.search_keyword_corpus(keyword, db_dict, case_sensitive, show_context, context_words)`\n",
    "4. perform keyword frequency analysis using `got3.keyword_frequency_analysis(keyword, db_dict, case_sensitive)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca4830",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
